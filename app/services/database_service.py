# V2 Database Service with pgvector support
"""
This service manages all interactions with the PostgreSQL database.

It uses the `asyncpg` library for asynchronous communication with the database.
Key features include:
- Connection pooling to manage database connections efficiently.
- CRUD (Create, Read, Update, Delete) operations for AI test logs.
- Vector similarity search using pgvector.
- Health check endpoint to verify database status.
"""

import logging
from typing import List, Dict, Any, Optional
import asyncpg
from datetime import datetime
from app.core.config import settings

logger = logging.getLogger(__name__)


class DatabaseService:
    """
    A service class to handle PostgreSQL operations, including pgvector functionality.

    Attributes:
        pool: An `asyncpg.Pool` instance for managing database connections.
    """

    def __init__(self):
        """Initializes the DatabaseService with a None value for the connection pool."""
        self.pool: Optional[asyncpg.Pool] = None

    async def initialize(self):
        """
        Initializes the asynchronous database connection pool.

        This method should be called during application startup.
        It uses settings from the application's configuration.

        Raises:
            RuntimeError: If the database pool fails to initialize.
        """
        if self.pool:
            logger.warning("Database pool already initialized.")
            return

        logger.info(f"Initializing database connection pool to {settings.DATABASE_URL}...")
        try:
            self.pool = await asyncpg.create_pool(
                dsn=settings.DATABASE_URL,
                min_size=settings.DB_POOL_MIN_SIZE,
                max_size=settings.DB_POOL_MAX_SIZE,
                command_timeout=settings.DB_COMMAND_TIMEOUT
            )
            logger.info(f"Database connection pool initialized successfully with min={settings.DB_POOL_MIN_SIZE}, max={settings.DB_POOL_MAX_SIZE} connections")
        except Exception as e:
            logger.critical(f"Failed to initialize database pool: {e}", exc_info=True)
            raise RuntimeError(f"Database initialization failed: {e}")

    async def close(self):
        """Closes the database connection pool gracefully."""
        if self.pool:
            logger.info("Closing database connection pool...")
            await self.pool.close()
            self.pool = None
            logger.info("Database connection pool closed successfully.")
        else:
            logger.debug("Database pool was not initialized, nothing to close.")

    async def create_ai_log(
        self, system_prompt: str, user_context: str, ai_result: str,
        embedding: List[float], file_url: Optional[str] = None,
        response_time_ms: Optional[int] = None
    ) -> Dict[str, Any]:
        """
        Inserts a new AI test log entry into the database.

        Args:
            system_prompt: The system prompt used for the AI.
            user_context: The user's input or context.
            ai_result: The response generated by the AI.
            embedding: The vector embedding of the user context.
            file_url: An optional URL to an associated file in object storage.
            response_time_ms: An optional value for the response time in milliseconds.

        Returns:
            A dictionary representing the newly created log entry.

        Raises:
            RuntimeError: If the database operation fails.
        """
        logger.debug(f"Creating AI log entry with embedding dimensions: {len(embedding)}")
        try:
            async with self.pool.acquire() as conn:
                logger.debug("Acquired database connection from pool")
                # Convert the embedding list to a PostgreSQL vector string format
                # pgvector expects format like '[0.1, 0.2, 0.3]'
                embedding_str = f"[{','.join(map(str, embedding))}]"

                result = await conn.fetchrow("""
                    INSERT INTO ai_test_logs
                    (system_prompt, user_context, ai_result, embedding, file_url, response_time_ms)
                    VALUES ($1, $2, $3, $4::vector, $5, $6)
                    RETURNING id, created_at
                """, system_prompt, user_context, ai_result, embedding_str, file_url, response_time_ms)

                log_entry = dict(result)
                logger.info(f"Successfully created AI log entry with ID: {log_entry['id']}, response_time: {response_time_ms}ms")
                return log_entry
        except Exception as e:
            logger.error(f"Error creating AI log: {e}", exc_info=True)
            raise RuntimeError(f"Failed to create AI log in database: {e}")

    async def find_similar_logs(
        self, embedding: List[float], limit: int = 5, min_similarity: float = 0.5
    ) -> List[Dict[str, Any]]:
        """
        Finds similar AI logs based on vector cosine similarity.

        Args:
            embedding: The vector embedding to search with.
            limit: The maximum number of similar logs to return.
            min_similarity: The minimum similarity score (from 0.0 to 1.0) to include in results.

        Returns:
            A list of similar log entries, each including a 'similarity' score.

        Raises:
            RuntimeError: If the database search fails.
        """
        logger.debug(f"Searching for similar logs with embedding dimensions: {len(embedding)}, min_similarity: {min_similarity}, limit: {limit}")
        try:
            async with self.pool.acquire() as conn:
                logger.debug("Executing vector similarity search query...")
                # Convert the embedding list to PostgreSQL vector string format
                embedding_str = f"[{','.join(map(str, embedding))}]"

                # The `<=>` operator calculates the cosine distance (0=identical, 2=opposite).
                # We subtract from 1 to get cosine similarity (1=identical, -1=opposite).
                results = await conn.fetch("""
                    SELECT id, system_prompt, user_context, ai_result, file_url,
                           response_time_ms, created_at,
                           1 - (embedding <=> $1::vector) as similarity
                    FROM ai_test_logs
                    WHERE 1 - (embedding <=> $1::vector) >= $2
                    ORDER BY similarity DESC -- Order by similarity descending
                    LIMIT $3
                """, embedding_str, min_similarity, limit)

                similar_logs = [dict(log) for log in results]
                logger.info(f"Vector search completed: Found {len(similar_logs)} similar logs with similarity >= {min_similarity}")
                if similar_logs:
                    top_similarity = similar_logs[0].get('similarity', 0)
                    logger.debug(f"Top similarity score: {top_similarity:.4f}")
                return similar_logs
        except Exception as e:
            logger.error(f"Error finding similar logs: {e}", exc_info=True)
            raise RuntimeError(f"Failed to execute vector similarity search: {e}")

    async def health_check(self) -> Dict[str, Any]:
        """
        Performs a health check on the database.

        This check verifies:
        - Basic connectivity.
        - The presence of the 'vector' extension.
        - The existence of the 'ai_test_logs' table.
        - The number of logs in the table.

        Returns:
            A dictionary containing the health status and diagnostic information.
        """
        if not self.pool:
            logger.warning("Database health check failed: Pool not initialized")
            return {"status": "error", "error": "Database pool not initialized"}

        logger.debug("Starting database health check...")
        try:
            async with self.pool.acquire() as conn:
                # 1. Test basic connectivity
                logger.debug("Testing database connectivity...")
                await conn.fetchval("SELECT 1")

                # 2. Test pgvector extension
                logger.debug("Checking pgvector extension...")
                vector_ext = await conn.fetchval("SELECT 1 FROM pg_extension WHERE extname = 'vector'")

                # 3. Test table existence
                logger.debug("Checking ai_test_logs table...")
                table_exists = await conn.fetchval("SELECT to_regclass('public.ai_test_logs')")

                health_status = {
                    "status": "healthy",
                    "connectivity": "ok",
                    "pgvector_extension": "installed" if vector_ext else "missing",
                    "ai_test_logs_table": "exists" if table_exists else "missing",
                }
                logger.info(f"Database health check completed: {health_status['status']}")
                return health_status
        except Exception as e:
            logger.error(f"Database health check failed: {e}", exc_info=True)
            return {"status": "error", "error": str(e)}

# Create a single, global instance of the DatabaseService.
# This instance will be imported and used by other parts of the application.


database_service = DatabaseService()
